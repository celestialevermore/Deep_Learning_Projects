{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ratsnlp.nlpbook.classification import ClassificationTrainArguments\n",
    "\n",
    "args = ClassificationTrainArguments(\n",
    "    pretrained_model_name=\"beomi/kcbert-base\", \n",
    "    downstream_corpus_name=\"nsmc\", \n",
    "    downstream_corpus_root_dir=\"/home/key2317/main/BERT와GPT로 배우는 자연어처리/root/Korpora\",\n",
    "    downstream_model_dir=\"/home/key2317/main/BERT와GPT로 배우는 자연어처리/nlpbook/checkpoint-doccls\",\n",
    "    learning_rate=5e-5,\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 내려받기\n",
    "\n",
    "- 프리트레인을 이미 마친 모델을 다운스트림 데이터로 파인튜닝\n",
    "- 파인튜닝을 위한 다운스트림 태스크용 데이터를 미리 내려받습니다.\n",
    "- nsmc : 네이버 영화 리뷰 말뭉치\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nsmc] download ratings_train.txt: 14.6MB [00:11, 1.31MB/s]                            \n",
      "[nsmc] download ratings_test.txt: 4.90MB [00:00, 11.0MB/s]                            \n"
     ]
    }
   ],
   "source": [
    "from Korpora import Korpora\n",
    "\n",
    "Korpora.fetch(\n",
    "    corpus_name=args.downstream_corpus_name,\n",
    "    root_dir=args.downstream_corpus_root_dir,\n",
    "    force_download=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-trained 모델 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at beomi/kcbert-base were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig,BertForSequenceClassification\n",
    "\n",
    "pretrained_model_config = BertConfig.from_pretrained(\n",
    "    args.pretrained_model_name,\n",
    "    num_labels=2,\n",
    ")\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    args.pretrained_model_name,\n",
    "    config=pretrained_model_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "토크나이저 준비\n",
    "\n",
    "- 문장을 토큰 시퀀스로 분석하는 과정을 토큰화라 합니다.\n",
    "- 그리고 그 토큰화를 수행하는 프로그램을 토크나이저라고 합니다.\n",
    "- kcbert-base 모델이 사용하는 토크나이저를 준비하는 코드입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer \n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    args.pretrained_model_name,\n",
    "    do_lower_case=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 로더 준비하기\n",
    "\n",
    "- Pytorch는 딥러닝 모델 학습을 지원하는 파이썬 라이브러리입니다.\n",
    "- Pytorch에는 데이터 로더라는게 포함되어 있습니다. 파이토치로 딥러닝 모델을 만들려면 이 데이터로더를 반드시 정의해야 합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터 로더\n",
    "\n",
    "  - 데이터를 배치 단위로 모델에 밀어 넣어주는 역할\n",
    "  - 전체 데이터 중 일부를 뽑아(sample) 배치를 구성합니다.\n",
    "  - 데이터셋은 데이터 로더의 구성 요소 중 하나입니다. 데이터셋은 여러 인스턴스(문서+레이블)을 포함하고 있습니다. \n",
    "  - 데이터 로더가 배치를 만들 때 인스턴스를 뽑는 방식은, 파이토치 사용자가 자유롭게 정할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 배치의 모양은 고정적인 경우가 많습니다. \n",
    "- 다시 말해, 동일한 배치에 있는 문장들의 토큰 개수는 서로 같아야 합니다.\n",
    "- 따라서 배치가 3인 인풋에서 각각 토큰의 길이가 5,3,4라면, 3,4의 길이를 5와 맞추기 위해 0을 삽입하게 됩니다.\n",
    "- 이 때 이 삽입된 0 token을 padding token이라고 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문서 분류 데이터 로더 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlpbook' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22491/2039695038.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnlpbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_collator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mdrop_last\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu_workers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nlpbook' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from ratsnlp.nlpbook.classification import NsmcCorpus,ClassificationDataset \n",
    "\n",
    "corpus = NsmcCorpus()\n",
    "\n",
    "train_dataset = ClassificationDataset(\n",
    "    args=args,\n",
    "    corpus=corpus,\n",
    "    tokenizer=tokenizer,\n",
    "    mode=\"train\",\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size = args.batch_size,\n",
    "    sampler = RandomSampler(train_dataset,replacement=False), \n",
    "    collate_fn=nlpbook.data_collator,\n",
    "    drop_last=False,\n",
    "    num_workers=args.cpu_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b45cb9130eebfe057782ef3fcf20eb5ae5c81aa7ea2b32ada9dc8e4e9a15b7c6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
