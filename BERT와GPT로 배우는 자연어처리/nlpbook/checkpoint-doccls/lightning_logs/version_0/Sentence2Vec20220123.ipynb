{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문장을 벡터로 변환하기\n",
    "\n",
    "여기에서는 프리트레인을 마친 BERT 모델에 문장을 입력하여\n",
    "\n",
    "이를 벡터로 변환하는 실습을 진행합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 토크나이저 선언\n",
    "\n",
    "입력값을 만들려면 토크나이저부터 선언해 두어야 합니다. \n",
    "\n",
    "다음 코드를 실행하면 kcbert-base 모델이 쓰는 토크나이저를 선언합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained( \n",
    "    \"beomi/kcbert-base\",\n",
    "    do_lower_case=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 모델 초기화\n",
    "\n",
    "BERT 모델이 프리트레인할 때 썼던 토크나이저를 그대로 사용해야 벡터 변환에 문제가 없습니다. \n",
    "\n",
    "모델을 선언할 때 앞 코드와 똑같은 모델 이름을 적용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at beomi/kcbert-base were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig,BertModel \n",
    "\n",
    "pretrained_model_config = BertConfig.from_pretrained( \n",
    "    \"beomi/kcbert-base\"\n",
    ")\n",
    "\n",
    "model = BertModel.from_pretrained( \n",
    "    \"beomi/kcbert-base\",\n",
    "    config=pretrained_model_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"beomi/kcbert-base\",\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"directionality\": \"bidi\",\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 300,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pooler_fc_size\": 768,\n",
       "  \"pooler_num_attention_heads\": 12,\n",
       "  \"pooler_num_fc_layers\": 3,\n",
       "  \"pooler_size_per_head\": 128,\n",
       "  \"pooler_type\": \"first_token_transform\",\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.10.0\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30000\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pretrained_model_config에는 BERT 모델을 프리트레인할 때 설정했던 내용이 있습니다.\n",
    "\n",
    "블록(레이어) 수는 12개, 헤드 수는 12개, 어휘 집합의 크기는 30000개 등 정보를 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이러한 설정에 따라 모델 전체를 초기화한 뒤 미리 학습된 kcbert-base를 체크포인트로 읽어들이는 역할을 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 입력값 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=[\"안녕하세요\",\"하이!\",\"섹스 하고 싶다\",\"개새끼야\"]\n",
    "\n",
    "features=tokenizer( \n",
    "    sentences,\n",
    "    max_length=10, \n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "코드를 실행하고 feature의 내용을 확인해 보면 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[2, 19017, 8482, 3, 0, 0, 0, 0, 0, 0], [2, 15830, 5, 3, 0, 0, 0, 0, 0, 0], [2, 23135, 8039, 8975, 3, 0, 0, 0, 0, 0], [2, 26998, 15028, 3, 0, 0, 0, 0, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0], [1, 1, 1, 1, 0, 0, 0, 0, 0, 0]]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input_ids : 4개의 입력 문장 각각에 대해 워드피스 토큰화를 수행한 뒤 이를 토큰 인덱스로 변환한 결과\n",
    "\n",
    "시작점이 2이고 끝점이 3으로 만들었고, padding은 max length에 맞춰 0을 입력하여 나중에 연산에 이제 쓰이지 않도록 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "토큰 최대 길이를 10으로 설정하고, \n",
    "\n",
    "토큰 길이가 이보다 짧으면 0으로, 길면 trucation = true이므로 자르도록 합니다. \n",
    "\n",
    "따라서 input_ids는 모두 10입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한편 atention_mask는 패딩이 아닌 토큰이 1, 패딩인 토큰이 0으로 실제 토큰이 자리하는지 아닌지에 대한 \n",
    "\n",
    "boolean성 정보를 제공합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "token_type_ids\n",
    "\n",
    "세그먼트 정보로 지금처럼 각각이 1개의 문장으로 구성되어있을 떄에는 0입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. BERT로 단어/문장 수준 벡터 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "피쳐를 토치 텐서로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {k: torch.tensor(v) for k,v in features.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2, 19017,  8482,     3,     0,     0,     0,     0,     0,     0],\n",
       "         [    2, 15830,     5,     3,     0,     0,     0,     0,     0,     0],\n",
       "         [    2, 23135,  8039,  8975,     3,     0,     0,     0,     0,     0],\n",
       "         [    2, 26998, 15028,     3,     0,     0,     0,     0,     0,     0]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 코드를 실행하여 BERT 모델을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 코드의 실행 결과인 outputs는 BERT 모델의 여러 출력 결과를 한데 묶은 것입니다. \n",
    "\n",
    "이는 문장 4개의 입력 토큰 각각에 해당하는 BERT의 마지막 레이어 출력 벡터들입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-6.9690e-01, -8.2478e-01,  1.7512e+00,  ..., -3.7320e-01,\n",
       "           7.3986e-01,  1.1907e+00],\n",
       "         [-1.4803e+00, -4.3981e-01,  9.4438e-01,  ..., -7.4049e-01,\n",
       "          -2.1058e-02,  1.3064e+00],\n",
       "         [-1.4299e+00, -5.0328e-01, -2.0686e-01,  ...,  1.2845e-01,\n",
       "          -2.6110e-01,  1.6057e+00],\n",
       "         ...,\n",
       "         [-1.4406e+00,  3.4313e-01,  1.4043e+00,  ..., -5.6484e-02,\n",
       "           8.4503e-01, -2.1696e-01],\n",
       "         [-1.3625e+00, -2.4036e-01,  1.1757e+00,  ...,  8.8759e-01,\n",
       "          -1.0535e-01,  7.3388e-02],\n",
       "         [-1.4244e+00,  1.5180e-01,  1.2920e+00,  ...,  2.4498e-02,\n",
       "           7.5718e-01,  7.9514e-03]],\n",
       "\n",
       "        [[ 9.3707e-01, -1.4749e+00,  1.7351e+00,  ..., -3.4262e-01,\n",
       "           8.0505e-01,  4.0307e-01],\n",
       "         [ 1.6095e+00, -1.7269e+00,  2.7936e+00,  ...,  3.1002e-01,\n",
       "          -4.7874e-01, -1.2491e+00],\n",
       "         [ 4.8612e-01, -4.5688e-01,  5.7122e-01,  ..., -1.7690e-01,\n",
       "           1.1253e+00, -2.7557e-01],\n",
       "         ...,\n",
       "         [ 1.2362e+00, -6.1808e-01,  2.0906e+00,  ...,  1.3677e+00,\n",
       "           8.1319e-01, -2.7417e-01],\n",
       "         [ 5.4088e-01, -9.6523e-01,  1.6237e+00,  ...,  1.2395e+00,\n",
       "           9.1854e-01,  1.7824e-01],\n",
       "         [ 1.9001e+00, -5.8595e-01,  3.0156e+00,  ...,  1.4967e+00,\n",
       "           1.9241e-01, -4.4479e-01]],\n",
       "\n",
       "        [[ 6.3524e-01, -1.8042e-01,  1.6321e+00,  ..., -7.7229e-04,\n",
       "           2.4531e-01,  1.0744e+00],\n",
       "         [-5.8102e-01, -1.3804e-01,  1.8093e+00,  ...,  2.8416e-01,\n",
       "          -8.4216e-01, -1.1181e+00],\n",
       "         [ 4.2432e-01, -7.1131e-02,  4.9444e-01,  ..., -7.3899e-01,\n",
       "           1.7691e-01,  1.2331e+00],\n",
       "         ...,\n",
       "         [-6.2268e-01,  1.0395e+00,  1.5688e+00,  ..., -3.6911e-01,\n",
       "           3.9541e-01,  5.3363e-01],\n",
       "         [ 2.8709e-01,  1.0565e+00,  8.8964e-01,  ..., -4.6253e-02,\n",
       "          -5.2513e-02,  7.7979e-01],\n",
       "         [-2.9988e-01,  7.7727e-01,  1.5553e+00,  ..., -4.8723e-01,\n",
       "           6.0151e-02, -5.4138e-01]],\n",
       "\n",
       "        [[-2.6827e-01, -6.4202e-01,  1.5086e+00,  ..., -8.9739e-01,\n",
       "           5.3565e-01,  1.2838e+00],\n",
       "         [-5.1442e-01, -8.5607e-01,  1.5004e+00,  ..., -7.7269e-01,\n",
       "          -4.2994e-01, -3.5066e-01],\n",
       "         [-1.6000e-01, -4.0689e-01,  6.9822e-01,  ..., -1.2066e+00,\n",
       "          -4.3251e-01,  2.4263e-01],\n",
       "         ...,\n",
       "         [-2.5321e-01, -7.7828e-01,  1.8659e+00,  ...,  4.1195e-01,\n",
       "          -2.6842e-01, -1.2147e-01],\n",
       "         [-9.8975e-01, -6.4993e-01,  1.3005e+00,  ..., -1.0177e-01,\n",
       "          -3.0095e-01, -4.4387e-01],\n",
       "         [-6.2123e-01, -5.8130e-01,  1.6390e+00,  ...,  9.0017e-02,\n",
       "          -3.2427e-01, -4.0203e-01]]], grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.1594,  0.0547,  0.1101,  ...,  0.2684,  0.1596, -0.9828],\n",
       "        [-0.9221,  0.2969, -0.0110,  ...,  0.4291,  0.0311, -0.9955],\n",
       "        [-0.7335,  0.3168, -0.0450,  ...,  0.4299,  0.0854, -0.9903],\n",
       "        [ 0.4405, -0.0262, -0.1344,  ..., -0.1273,  0.0095, -0.9968]],\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10, 768])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1594,  0.0547,  0.1101,  ...,  0.2684,  0.1596, -0.9828],\n",
       "        [-0.9221,  0.2969, -0.0110,  ...,  0.4291,  0.0311, -0.9955],\n",
       "        [-0.7335,  0.3168, -0.0450,  ...,  0.4299,  0.0854, -0.9903],\n",
       "        [ 0.4405, -0.0262, -0.1344,  ..., -0.1273,  0.0095, -0.9968]],\n",
       "       grad_fn=<TanhBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.pooler_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 768])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.pooler_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bbacf2a9d2e876d097b37d768b1e7f504d43ac1a5c7cfa4c05142076e87a32d3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('tensorflowstart': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
