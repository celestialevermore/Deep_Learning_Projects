{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention is all you need\n",
    "\n",
    "- 본 코드는 Attention is all you need : Transformer의 논문 내용과 일부 기타 자료를 참고하여 작성하였습니다.\n",
    "\n",
    "- Tensorflow 대신 pytorch로 작성하였습니다.\n",
    "\n",
    "- 모델에 대한 학습은 아직 진행해보지는 않았습니다. 추후 진행 예정 -> 변수 명에 오류가 있음\n",
    "\n",
    "- 대신 Encoder, Decoder / Multi-head-attention / Residual connection(잔여 학습) + Normalization / Scaled-Dot-Product attention 구현은 하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!python -m spacy download en \n",
    "!python -m spacy download de "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-22 19:34:17.986223: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-22 19:34:17.986260: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import spacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "spacy_de = spacy.load(\"de_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인덱스 0 : I\n",
      "인덱스 1 : am\n",
      "인덱스 2 : a\n",
      "인덱스 3 : graduate\n",
      "인덱스 4 : student\n",
      "인덱스 5 : .\n"
     ]
    }
   ],
   "source": [
    "#간단히 토큰화 기능 써보기\n",
    "tokenized = spacy_en.tokenizer(\"I am a graduate student.\")\n",
    "\n",
    "for i, token in enumerate(tokenized):\n",
    "    print(\"인덱스\",i,':',token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 영어(English) 및 독일어(Deutsch) 토큰화 함수를 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 독일어 문장을 토큰화하는 함수 \n",
    "def tokenize_deutsch(text):\n",
    "    return [token.text for token in spacy_de.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영어 문장을 토큰화하는 함수\n",
    "def tokenize_english(text):\n",
    "    return [token.text for token in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 필드 라이브러리를 이용하여 데이터셋에 대한 구체적인 전처리 내용을 명시합니다.\n",
    "- Seq2Seq 모델과는 다르게 batch_first 속성의 값을 True로 설정합니다.\n",
    "- 번역 목표\n",
    "  - 소스(src) : 독일어\n",
    "  - 목표(trg) : 영어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Field,BucketIterator \n",
    "\n",
    "# Field : 어떻게 데이터를 전처리할 것인지를 설명 \n",
    "# 보통 소문자로 \n",
    "SRC = Field(tokenize=tokenize_deutsch, init_token=\"<sos>\",eos_token=\"<eos>\",lower=True,batch_first=True)\n",
    "TRG = Field(tokenize=tokenize_english,init_token=\"<sos>\",eos_token=\"<eos>\",lower=True,batch_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 대표적인 영어-독일어 번역 데이터셋인 Multi30k를 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import Multi30k \n",
    "\n",
    "train_dataset,valid_dataset,test_dataset = Multi30k.splits(exts=(\".de\",\".en\"),fields=(SRC,TRG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터셋(training dataset) 크기 : 29000\n",
      "평가 데이터셋(validation dataset) 크기 : 1014\n",
      "검증 데이터셋(testing dataset) 크기 : 1000\n"
     ]
    }
   ],
   "source": [
    "print(\"학습 데이터셋(training dataset) 크기 :\",len(train_dataset.examples))\n",
    "print(\"평가 데이터셋(validation dataset) 크기 :\",len(valid_dataset.examples))\n",
    "print(\"검증 데이터셋(testing dataset) 크기 :\",len(test_dataset.examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 한번 시험을 거쳐보도록 하겠습니다.\n",
    "  - 훈련 데이터셋의 20번째 독일어 문장(src)가 주어지면, \n",
    "  - 이에 대한 훈련 데이터셋의 20번째 영어 문장(trg)의 쌍이 주어지게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ein', 'großes', 'bauwerk', 'ist', 'kaputt', 'gegangen', 'und', 'liegt', 'auf', 'einer', 'fahrbahn', '.']\n",
      "['a', 'large', 'structure', 'has', 'broken', 'and', 'is', 'laying', 'in', 'a', 'roadway', '.']\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_dataset.examples[20])['src'])\n",
    "print(vars(train_dataset.examples[20])['trg'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 필드(Field) : build_vocab을 이용, 영어 / 독일어 단어 사전을 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_dataset,min_freq=2)\n",
    "TRG.build_vocab(train_dataset,min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "독일어 단어 차원 : 7853\n",
      "영어 단어 차원 : 5893\n"
     ]
    }
   ],
   "source": [
    "print('독일어 단어 차원 :',len(SRC.vocab))\n",
    "print('영어 단어 차원 :',len(TRG.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TRG.vocab, 단어 군집 안에서 특정 문자를 입력하면 그 문자에 해당하는 index를 반환\n",
    "- 이를 위해 stoi 내장 함수를 사용합니다.\n",
    "- 만일 없는 단어라면, 0 / <sos> : 2 / <eos> : 3에 해당하고, 실제로 존재하는 어떤 단어의 경우 그에 대한 인덱스를 뱉어냅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4112\n",
      "1752\n"
     ]
    }
   ],
   "source": [
    "print(TRG.vocab.stoi[\"abcabc\"]) #특정 string이 들어왔을 때 그에 대한 인덱스를 반환\n",
    "print(TRG.vocab.stoi[TRG.pad_token])\n",
    "print(TRG.vocab.stoi[\"<sos>\"])\n",
    "print(TRG.vocab.stoi[\"<eos>\"])\n",
    "print(TRG.vocab.stoi[\"hello\"])\n",
    "print(TRG.vocab.stoi[\"world\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 한 문장에 포함된 단어가 순서대로 나열된 상태로 네트워크에 입력되어야 합니다.\n",
    "\n",
    "  - 즉, 한 배치에 포함된 문장들이 가지는 단어의 개수에 큰 오차가 없도록 만들면 좋습니다.\n",
    "     - 그러니까, 배치의 크기를 거의 균일하게 만든다고 보면 됩니다.\n",
    "     - 다시 말해, 하나의 배치 안에 들어 있는 문장들의 Sequence 길이가 유사하도록 만들어서 \n",
    "     - 가능한 한 길이가 짧은 문장의 padding token을 적게 들어가도록 만들어줍니다. \n",
    "     - 실제로 학습을 위해 네트워크에 들어가는 데이터의 차원을 최적화할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - 이를 위해 BucketIlterator를 사용합니다.\n",
    "  - Batch Size는 128로 임의로 잡도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "#연구실 서버 상 제게 할당된 6번 서버에서 운용됩니다.\n",
    "#각자의 상황에 맞게 :n을 조절하시면 될 듯 합니다.\n",
    "device = torch.device('cuda:6' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "BATCH_SIZE = 128 \n",
    "\n",
    "train_iterator,valid_iterator,test_iterator = BucketIterator.splits( \n",
    "    (train_dataset,valid_dataset,test_dataset), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    device=device\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 각 Batch마다 확인하므로 시간복잡도가 n^2이 됩니다.\n",
    "- 따라서 여건 상 첫 번째 Batch만 확인하도록 하겠습니다.\n",
    "- (128,28) shape의 배치가 만들어 집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첫 번째 배치 크기 : torch.Size([128, 28])\n",
      "인덱스  0 : 2\n",
      "인덱스  1 : 5\n",
      "인덱스  2 : 25\n",
      "인덱스  3 : 7\n",
      "인덱스  4 : 6\n",
      "인덱스  5 : 102\n",
      "인덱스  6 : 882\n",
      "인덱스  7 : 297\n",
      "인덱스  8 : 12\n",
      "인덱스  9 : 14\n",
      "인덱스  10 : 136\n",
      "인덱스  11 : 11\n",
      "인덱스  12 : 6\n",
      "인덱스  13 : 2255\n",
      "인덱스  14 : 20\n",
      "인덱스  15 : 86\n",
      "인덱스  16 : 4\n",
      "인덱스  17 : 3\n",
      "인덱스  18 : 1\n",
      "인덱스  19 : 1\n",
      "인덱스  20 : 1\n",
      "인덱스  21 : 1\n",
      "인덱스  22 : 1\n",
      "인덱스  23 : 1\n",
      "인덱스  24 : 1\n",
      "인덱스  25 : 1\n",
      "인덱스  26 : 1\n",
      "인덱스  27 : 1\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(train_iterator):\n",
    "    ssrc = batch.src\n",
    "    ttrg = batch.trg \n",
    "\n",
    "    print(\"첫 번째 배치 크기 :\",ssrc.shape)\n",
    "\n",
    "    for i in range(ssrc.shape[1]):\n",
    "        print(\"인덱스 \",i,\":\", ssrc[0][i].item())\n",
    "\n",
    "    break \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 하나의 Batch를 확인했더니..\n",
    "- 1 : 의미가 없는 padding token에 해당합니다.\n",
    "- 2 : <sos>에 해당합니다.\n",
    "- 3 : <eos>에 해당합니다.\n",
    "\n",
    "- 즉 이 문장은 총 길이가 18인 하나의 문장을 뜻합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer : RNN, CNN을 쓰지 않고, Attention(self-attention / Multi-head-attention / Scaled-dot product attention)으로 이루어집니다.\n",
    "\n",
    "- 이를 논문의 내용과 유사하게 구현해보도록 하겠습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Attention을 이루는 세 가지 구성 요소\n",
    "  - Query : 하나의 Sequence에서 나와 가장 관련이 있는 단어가 무엇인지 묻는 주체(Subject)\n",
    "  - Key : 해당 Query가 바라보는 대상(본인 포함. 그래서 self-attention)\n",
    "  - Value : 소맥을 통과한 이후 가중값 계산을 위해 곱해지는 W\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hyperparameter\n",
    "  - hidden_dim : 하나의 단어에 대한 임베딩 차원. 즉 len(words[0])\n",
    "  - n_heads : head의 개수. multi-head attention에서 decoder에서 만들어 내는 head의 개수를 말합니다. 많아지면 많아질 수록 더욱 세밀한 연산이 가능합니다. \n",
    "    - 다른 표현으로 head의 개수는 scaled dot-product attention의 개수를 말하기도 합니다.\n",
    "  - dropout_ratio : Fully-connected layer를 통과할 때 임의의 한 Layer의 노드 집합 가운데에서 전체 노드 대비 비활성화되는 노드의 비율  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 임베딩 차원을 head의 개수로 나눠주게 되면 각각 Q,K,V의 가로 차원이 나오게 됩니다.\n",
    "- 세로 길이는 임베딩 차원이 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "\n",
    "class Multi_Head_Attention_Layer(nn.Module):\n",
    "\n",
    "    #생성자 결정하고 각각의 parameter 순서를 정해줍니다. \n",
    "\n",
    "    def __init__(self, hidden_dim,n_heads,dropout_ratio,device):\n",
    "        super().__init()\n",
    "        #예외 처리\n",
    "        assert hidden_dim % n_heads ==0\n",
    "\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        #임베딩 차원. 즉 한 문장에 대해 쭉 늘어뜨리는 정도 \n",
    "        self.n_heads = n_heads \n",
    "        #헤드의 개수. 임베딩 차원을 헤드의 개수로 나누면 Q,K,V의 차원이 나오게 된다.\n",
    "        self.head_dim = hidden_dim//n_heads \n",
    "        #즉 각 헤드에서의 임베딩 차원이 됩니다. \n",
    "\n",
    "        self.FC_Q = nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.FC_K = nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.FC_V = nn.Linear(hidden_dim,hidden_dim)\n",
    "        #Q,K,V에 적용될 각각의 레이어의 차원을 넣어줍니다. \n",
    "\n",
    "        self.FC_O = nn.Linear(hidden_dim,hidden_dim)\n",
    "        #마지막에 elementwise하게 연산될 것.\n",
    "\n",
    "        self.drop_out = nn.Dropout(dropout_ratio)\n",
    "        #하이퍼 파라미터\n",
    "\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "    \n",
    "\n",
    "\n",
    "    #FFW \n",
    "    def forward(self,q,k,v,mask=None):\n",
    "        batch_size = q.shape[0]\n",
    "\n",
    "        #원래 각각의 Q,K,V는 2차원이었으나, Batch 크기만큼 들어오게 되므로\n",
    "        #일괄적으로 Batch size 만큼 차원이 늘어나게 될 것\n",
    "\n",
    "        #q.shape : (batch_size,len(q),hidden_dim)\n",
    "        #k.shape : (batch_size,len(k),hidden_dim)\n",
    "        #v.shape : (batch_size,len(v),hidden_dim)\n",
    "\n",
    "        WQ = self.FC_Q(q)\n",
    "        WK = self.FC_K(k)\n",
    "        WV = self.FC_V(v)\n",
    "\n",
    "        #각 헤드에 대한 연산을 위하여\n",
    "        #기존의 hidden_dim을 n_heads * head_dims 로 변경 쉽게 말하면 쪼개준다\n",
    "        #n_heads(h) 개의 서로 다른 attention만큼 학습을 진행합니다. \n",
    "\n",
    "        WQ = WQ.view(batch_size,-1,self.n_heads,self.head_dim).permute(0,2,1,3)\n",
    "        WK = WK.view(batch_size,-1,self.n_heads,self.head_dim).permute(0,2,1,3)\n",
    "        WV = WV.view(batch_size,-1,self.n_heads,self.head_dim).permute(0,2,1,3)\n",
    "        #head의 개수만큼으로 나눠서 벡터들을 생성하고 길이는 head의 차원으로 설정합니다. \n",
    "\n",
    "        # Q.shape : [batch_size,n_heads,len(q),head_dim]\n",
    "        # K.shape : [batch_size,n_heads,len(k),head_dim]\n",
    "        # V.shape : [batch_size,n_heads,len(v),head_dim]\n",
    "\n",
    "        # Attention Energy 계산\n",
    "\n",
    "        each_energy = torch.matmul(WQ,WK.permute(0,1,3,2)) / self.scale \n",
    "\n",
    "        #에너지도 동일한 차원으로 맞춰준다. \n",
    "        #이제 이에 대한 소맥을 취한다. \n",
    "\n",
    "        if mask is not None:\n",
    "            #mask 처리된 부분이 소맥에 거의 반영되지 않도록 \n",
    "            each_energy = each_energy.masked_fill(mask==0,-1e10)\n",
    "        \n",
    "\n",
    "        #이제 어텐션 스코어를 각각 단어에 대해 계산합니다. \n",
    "        attention = torch.softmax(each_energy,dim=-1)\n",
    "\n",
    "        #attenion의 차원 : [batch_size,n_heads,len(q),len(k)]\n",
    "\n",
    "        #현재 Scaled-dot product Attention을 계산하는 중임 그 벤다이어그램을 생각하셈\n",
    "        res = torch.matmul(self.dropout(attention),WV)\n",
    "\n",
    "        res = res.permute(0,2,1,3).contiguous()\n",
    "\n",
    "        #한꺼번에 쭉 늘어뜨려 Concat의 효과를 유도\n",
    "        res = res.view(batch_size,-1,self.hidden_dim)\n",
    "        \n",
    "        res = self.FC_O(res)\n",
    "\n",
    "        return res,attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Position-wise Feedforward Architecture\n",
    "\n",
    "\n",
    "- 입력.shape와 출력.shape는 동일합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 하이퍼 파라미터\n",
    "  - hidden_dim : 하나의 단어의 임베딩 길이(차원)\n",
    "  - PFRD_dim : FFW Layer에서의 임베딩 길이(차원)\n",
    "  - dropout_ratio : 드롭아웃의 비율 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Position_Wise_Feed_Forward_Layer(nn.Module):\n",
    "    def __init__(self,hidden_dim,PFRD_dim,dropout_ratio):\n",
    "        super().__init__() \n",
    "\n",
    "        self.FC_1 = nn.Linear(hidden_dim,PFRD_dim)\n",
    "        self.FC_2 = nn.Linear(PFRD_dim,hidden_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "    \n",
    "    def Forward(self,res):\n",
    "        # 전부 선형대수연산임.\n",
    "        # res의 shape : [batch_size,len(seq),hidden_dim]\n",
    "\n",
    "        res = self.dropout(torch.relu(self.FC_1(res)))\n",
    "\n",
    "        # res의 shape : [batch_size,len(seq),PFRD_dim]\n",
    "\n",
    "        res = self.FC_2(res)\n",
    "\n",
    "        # res의 shape : [batch_size,len(seq),hidden_dim]\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 앞서 정의한 Attention과 FFN을 사용하여, Encoder / Decoder를 정의하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder \n",
    "\n",
    "- 낱개 Encoder를 정의\n",
    "  - premise : 입력.shape와 출력.shape가 동일\n",
    "  - 같은 구조의 Encoder를 중첩하여 사용합니다. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 총 4개의 Layer로 이루어져 있습니다.\n",
    "  - Multi-head Attention, Residual, FeedForward Layer, Residual\n",
    "  - Residual 잔여 연산은 LayerNorm(x+Sublayer(x)) 을 의미합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hyperparameter\n",
    "  - hidden_dim : 하나의 단어에 대한 임베딩 차원\n",
    "  - n_heads : head의 개수 == scaled-dot-product attention의 개수\n",
    "  - PFRD_dim : FFL에서의 임베딩 차원 \n",
    "  - dropout_ratio : 드롭아웃의 비율 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <pad> 토큰을 반영하지 않도록 마스크 값을 0으로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self,hidden_dim,n_heads,PFRD_dim,dropout_ratio,device):\n",
    "        super().__init__() \n",
    "    \n",
    "        self.self_attention_layer_norm = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "        self.Feed_Forward_Layer_Norm = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "        self.Self_Attention = Multi_Head_Attention_Layer(hidden_dim,n_heads,dropout_ratio,device)\n",
    "\n",
    "        self.Positionwise_Feedforward = Position_Wise_Feed_Forward_Layer(hidden_dim,PFRD_dim,dropout_ratio) \n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "\n",
    "    #하나의 임베딩이 복제되어 Q,K,V로 입력됨 \n",
    "    # 왜 복제를 하느냐하면 Residual 연산을 하기 위함. LayerNorm(x+Sublayer(x))\n",
    "\n",
    "    def Forward(self,src,src_mask):\n",
    "\n",
    "        #src : 우리는 지금 독일어 -> 영어 이므로 \n",
    "        #src mask : [batch_size,len(src)] \n",
    "        # \n",
    "\n",
    "        ################### Self Attention #################\n",
    "\n",
    "        #왜 src,src,src 이냐하면 Q,K,V만큼 복제를 해야 하기 때문\n",
    "        ssrc, tmp = self.Self_Attention(src,src,src,src_mask)    \n",
    "\n",
    "        #dropout, residual connection and Layer norm\n",
    "\n",
    "        src = self.self_attention_layer_norm(src+self.dropout(ssrc))\n",
    "\n",
    "        # src : [batch_size, len(src), hidden_dim]\n",
    "\n",
    "        #positionalwiseFFW\n",
    "        ssrc = self.Positionwise_Feedforward(src)\n",
    "\n",
    "        #dropout,residual and layer norm\n",
    "\n",
    "        src = self.Feed_Forward_Layer_Norm(src+self.dropout(ssrc))\n",
    "\n",
    "        #그냥 논문의 내용을 충실하게 구현중임.\n",
    "\n",
    "        return src "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인코더(Encoder) 전체를 구현합니다.\n",
    "\n",
    "- 논문에서 나온 대로 N개 만큼의 인코더를 구현합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 하이퍼파라미터\n",
    "  - input_dim : 하나의 단어에 대한 OHE 길이(차원)\n",
    "  - hidden_dim : 하나의 단어에 대한 임베딩 길이(차원)\n",
    "  - n_layers : 논문에서 나온 N을 의미, 즉 인코딩 레이어의 개수\n",
    "  - n_heads : Multi-head attention을 위한 head의 개수. head의 개수는 Scaled-dot-product attention의 개수과 같다.\n",
    "  - PFRD_dim : FFW 레이어에서의 임베딩 길이(차원)\n",
    "  - dropout_ratio : 드롭아웃 비율\n",
    "  - max_length : 문장 내 최대 단어의 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self,input_dim,hidden_dim,n_layers,n_heads,PFRD_dim,dropout_ratio,device,max_length=100):\n",
    "        super().__init__() \n",
    "\n",
    "        self.device = device \n",
    "\n",
    "        self.token_embedding = nn.Embedding(input_dim,hidden_dim)\n",
    "        #처음 인풋에 대한 OHE와, Positional Embeding을 위한 hidden_dim까지 \n",
    "\n",
    "        self.pos_embedding = nn.Embedding(max_length,hidden_dim)\n",
    "\n",
    "        self.Layers = nn.ModuleList([EncoderLayer(hidden_dim,n_heads,PFRD_dim,dropout_ratio,device) for i in range(n_layers)])\n",
    "        # N개의 Layer만큼 추가\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hidden_dim])).to(device)\n",
    "\n",
    "    def Forward(self,src,src_mask):\n",
    "\n",
    "        # src: [batch_size, src_len]\n",
    "        # src_mask: [batch_size, src_len]\n",
    "\n",
    "        batch_size = src.shape[0]\n",
    "        src_len = src.shape[1]\n",
    "\n",
    "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "\n",
    "        # pos: [batch_size, src_len]\n",
    "\n",
    "        # 소스 문장의 임베딩과 위치 임베딩을 더한 것을 사용\n",
    "        src = self.dropout((self.token_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
    "\n",
    "        # src: [batch_size, src_len, hidden_dim]\n",
    "\n",
    "        # 모든 인코더 레이어를 차례대로 거치면서 순전파(forward) 수행\n",
    "        for layer in self.Layers:\n",
    "            src = layer(src, src_mask)\n",
    "\n",
    "        # src: [batch_size, src_len, hidden_dim]\n",
    "\n",
    "        return src # 마지막 레이어의 출력을 반환\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder\n",
    "\n",
    "- 낱개의 디코더 레이어\n",
    "  - premise : 입력.shape == 출력.shape \n",
    "  - Encoder와 마찬가지로 동일한 n_layers만큼 중첩하여 사용합니다. \n",
    "  - Decoder에서는 Encoder의 ouput을 k,v로 사용할 수 있도록, Q 역할을 할 수 있는 또 다른 Multi-head-Attention layer가 있다.\n",
    "  - Multi-head-Attention 2개 / Residual + Normalization 2개 / Feed Forward 1개 / Residual + Normalization 1개 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 하이퍼 파라미터\n",
    "  - Encoder와 동일"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 타겟 문장에서 각 단어들은 다음 단어를 미리 알 수 없도록 만들기 위해 마스크를 사용합니다. \n",
    "  - 왜냐하면, Encoder에서 나온 문장 전체를 참고하게 되면 객관적인 모델 평가에 있어 악영향을 줄 수도 있기 때문입니다. \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_heads, pf_dim, dropout_ratio, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attention_layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.encoding_attention_layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.Feed_Forward_Layer_Norm = nn.LayerNorm(hidden_dim)\n",
    "        self.Self_Attention = Multi_Head_Attention_Layer(hidden_dim, n_heads, dropout_ratio, device)\n",
    "        self.Encoder_Attention = Multi_Head_Attention_Layer(hidden_dim, n_heads, dropout_ratio, device)\n",
    "        self.Positionwise_Feedforward = Position_Wise_Feed_Forward_Layer(hidden_dim, pf_dim, dropout_ratio)\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "    # 인코더의 출력 값(enc_src)을 어텐션(attention)하는 구조\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "\n",
    "        # trg: [batch_size, trg_len, hidden_dim]\n",
    "        # enc_src: [batch_size, src_len, hidden_dim]\n",
    "        # trg_mask: [batch_size, trg_len]\n",
    "        # src_mask: [batch_size, src_len]\n",
    "\n",
    "        # self attention\n",
    "        # 자기 자신에 대하여 어텐션(attention)\n",
    "        _trg, _ = self.Self_Attention(trg, trg, trg, trg_mask)\n",
    "\n",
    "        # dropout, residual connection and layer norm\n",
    "        trg = self.self_attention_layer_norm(trg + self.dropout(_trg))\n",
    "\n",
    "        # trg: [batch_size, trg_len, hidden_dim]\n",
    "\n",
    "        # encoder attention\n",
    "        # 디코더의 쿼리(Query)를 이용해 인코더를 어텐션(attention)\n",
    "        _trg, attention = self.Encoder_Attention(trg, enc_src, enc_src, src_mask)\n",
    "\n",
    "        # dropout, residual connection and layer norm\n",
    "        trg = self.encoding_attention_layer_norm(trg + self.dropout(_trg))\n",
    "\n",
    "        # trg: [batch_size, trg_len, hidden_dim]\n",
    "\n",
    "        # positionwise feedforward\n",
    "        _trg = self.Positionwise_Feedforward(trg)\n",
    "\n",
    "        # dropout, residual and layer norm\n",
    "        trg = self.Feed_Forward_Layer_Norm(trg + self.dropout(_trg))\n",
    "\n",
    "        # trg: [batch_size, trg_len, hidden_dim]\n",
    "        # attention: [batch_size, n_heads, trg_len, src_len]\n",
    "\n",
    "        return trg, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "디코더 전체\n",
    "\n",
    "- 하이퍼 파라미터\n",
    "  - output_dim : 하나의 단어에 대한 OHE 길이(차원)\n",
    "  - hidden_dim : 하나의 단어에 대한 임베딩 길이(차원)\n",
    "  - n_layers : 인코더의 개수 == 디코더의 개수\n",
    "  - n_heads : head의 개수 == Scaled Dot Product attention의 개수 \n",
    "  - PFRD_dim : FFW Layer에서의 임베딩 길이(차원)\n",
    "  - dropout_ratio : 드롭아웃 비율 \n",
    "  - max_length : 문장 내 최대 단어의 개수 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 타겟 문장에서 각 단어는 다음 단어가 무엇인지 알 수 없게 하였습니다..\n",
    "  - 임의의 t 위치에 대한 단어 output은 1부터 t-1까지만 확인할 수 있도록 처리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,output_dim,hidden_dim,n_layers,n_heads,PFRD_dim,dropout_ratio,device,max_length=100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.token_embedding = nn.Embedding(output_dim,hidden_dim)\n",
    "\n",
    "        self.pos_embedding = nn.Embedding(max_length,hidden_dim)\n",
    "\n",
    "        self.Layers = nn.ModuleList([DecoderLayer(hidden_dim,n_heads,PFRD_dim,dropout_ratio,device) for i in range(n_layers)])\n",
    "\n",
    "        self.FC_OUT = nn.Linear(hidden_dim,output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hidden_dim])).to(device)\n",
    "\n",
    "    \n",
    "    def Forward(self,trg,enc_src,trg_mask,src_mask):\n",
    "\n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len=trg.shape[1]\n",
    "\n",
    "        pos = torch.arange(0,trg_len).unsqueeze(0).repeat(batch_size,1).to(self.device)\n",
    "\n",
    "        trg = self.dropout((self.token_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
    "\n",
    "        # trg: [batch_size, trg_len, hidden_dim]\n",
    "\n",
    "        for layer in self.Layers:\n",
    "            # 소스 마스크와 타겟 마스크 모두 사용\n",
    "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
    "\n",
    "        # trg: [batch_size, trg_len, hidden_dim]\n",
    "        # attention: [batch_size, n_heads, trg_len, src_len]\n",
    "\n",
    "        output = self.FC_OUT(trg)\n",
    "\n",
    "        # output: [batch_size, trg_len, output_dim]\n",
    "\n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer\n",
    "\n",
    "- 전체 트랜스포머의 모델을 정의합니다.\n",
    "- 입력이 들어왔을 때 앞서 정의한 인코더와 디코더를 거쳐 출력 문장을 만들어냅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, Encoder, Decoder, src_pad_idx, trg_pad_idx, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.Encoder = Encoder\n",
    "        self.Decoder = Decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "\n",
    "    # 소스 문장의 <pad> 토큰에 대하여 마스크(mask) 값을 0으로 설정\n",
    "    def make_src_mask(self, src):\n",
    "\n",
    "        # src: [batch_size, src_len]\n",
    "\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        # src_mask: [batch_size, 1, 1, src_len]\n",
    "\n",
    "        return src_mask\n",
    "\n",
    "    # 타겟 문장에서 각 단어는 다음 단어가 무엇인지 알 수 없도록(이전 단어만 보도록) 만들기 위해 마스크를 사용\n",
    "    def make_trg_mask(self, trg):\n",
    "\n",
    "        # trg: [batch_size, trg_len]\n",
    "\n",
    "       \n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        # trg_pad_mask: [batch_size, 1, 1, trg_len]\n",
    "\n",
    "        trg_len = trg.shape[1]\n",
    "\n",
    "        \n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
    "\n",
    "        # trg_sub_mask: [trg_len, trg_len]\n",
    "\n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "\n",
    "        # trg_mask: [batch_size, 1, trg_len, trg_len]\n",
    "\n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "\n",
    "        # src: [batch_size, src_len]\n",
    "        # trg: [batch_size, trg_len]\n",
    "\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "\n",
    "        # src_mask: [batch_size, 1, 1, src_len]\n",
    "        # trg_mask: [batch_size, 1, trg_len, trg_len]\n",
    "\n",
    "        enc_src = self.Encoder(src, src_mask)\n",
    "\n",
    "        # enc_src: [batch_size, src_len, hidden_dim]\n",
    "\n",
    "        output, attention = self.Decoder(trg, enc_src, trg_mask, src_mask)\n",
    "\n",
    "        # output: [batch_size, trg_len, output_dim]\n",
    "        # attention: [batch_size, n_heads, trg_len, src_len]\n",
    "\n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b45cb9130eebfe057782ef3fcf20eb5ae5c81aa7ea2b32ada9dc8e4e9a15b7c6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
